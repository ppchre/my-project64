{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 23:40:28.038217: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-14 23:40:28.038412: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Define the main folder containing the dataset\n",
    "dataset_dir = 'FacialDataset'\n",
    "\n",
    "# Set the image size and batch size\n",
    "image_size = (256, 256)\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator with data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "def preprocess_image(file_path):\n",
    "    image = cv2.imread(file_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Unable to read image {file_path}. It may be corrupt or the path may be incorrect.\")\n",
    "        os.remove(file_path)\n",
    "        return\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(image_rgb)\n",
    "    \n",
    "    if results:\n",
    "        x1, y1, width, height = results[0]['box']\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        face = image_rgb[y1:y2, x1:x2]\n",
    "        face_array = cv2.resize(face, image_size)\n",
    "\n",
    "        mask = np.zeros((image_size[0], image_size[1]), dtype=\"uint8\")\n",
    "        resized_x1, resized_y1, resized_x2, resized_y2 = get_resized_coordinates(x1, y1, width, height, image_rgb)\n",
    "        cv2.rectangle(mask, (resized_x1, resized_y1), (resized_x2, resized_y2), 255, -1)\n",
    "\n",
    "        blurred_image = cv2.GaussianBlur(face_array, (21, 21), 0)\n",
    "        final_image = cv2.bitwise_and(blurred_image, blurred_image, mask=cv2.bitwise_not(mask))\n",
    "        final_image += cv2.bitwise_and(face_array, face_array, mask=mask)\n",
    "\n",
    "        final_image = cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Ensure the file has a valid image extension before writing\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        if file_extension not in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "            print(f\"Error: Unsupported file extension '{file_extension}' for file {file_path}.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            cv2.imwrite(file_path, final_image)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred while saving image {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"No face detected in image {file_path}. Removing it.\")\n",
    "        os.remove(file_path)\n",
    "\n",
    "def get_resized_coordinates(x1, y1, width, height, original_image):\n",
    "    resized_x1 = int(image_size[0] * x1 / original_image.shape[1])\n",
    "    resized_y1 = int(image_size[1] * y1 / original_image.shape[0])\n",
    "    resized_x2 = resized_x1 + int(image_size[0] * width / original_image.shape[1])\n",
    "    resized_y2 = resized_y1 + int(image_size[1] * height / original_image.shape[0])\n",
    "    return resized_x1, resized_y1, resized_x2, resized_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor class_name in os.listdir(dataset_dir):\\n    class_path = os.path.join(dataset_dir, class_name)\\n    if os.path.isdir(class_path):\\n        for image_filename in os.listdir(class_path):\\n            image_path = os.path.join(class_path, image_filename)\\n            preprocess_image(image_path)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for preprocessing\n",
    "'''\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for image_filename in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_filename)\n",
    "            preprocess_image(image_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for image_filename in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_filename)\n",
    "            if os.path.isfile(image_path):\n",
    "                X.append(image_path)\n",
    "                y.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Square Facial': 837 images\n",
      "Class 'Round Facial': 820 images\n",
      "Class 'Oval Facial': 799 images\n",
      "Class 'Heart Facial': 794 images\n",
      "Class 'Diamond Facial': 98 images\n",
      "Class 'Triangle Facial': 97 images\n",
      "Class 'Oblong Facial': 793 images\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class_distribution = Counter(y)\n",
    "\n",
    "for class_name, count in class_distribution.items():\n",
    "    print(f\"Class '{class_name}': {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3390 non-validated image filenames belonging to 7 classes.\n",
      "Found 848 non-validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'filename': X_train, 'class': y_train}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validate_filenames=False  # Disable filename validation for better performance\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'filename': X_test, 'class': y_test}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    validate_filenames=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(256, 256, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "\n",
    "    keras.layers.Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "labels = list(set(y))\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "y_integers = np.array([class_indices[class_name] for class_name in y])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    if epoch < 5:\n",
    "        return initial_lr\n",
    "    else:\n",
    "        return initial_lr * tf.math.exp(0.1 * (5 - epoch))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 50 \n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[lr_scheduler, early_stopping],\n",
    "    verbose=1,\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('facial_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRETRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3351 validated image filenames belonging to 7 classes.\n",
      "Found 840 validated image filenames belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yo/miniforge3/envs/mlp/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 39 invalid image filename(s) in x_col=\"filename\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/Users/yo/miniforge3/envs/mlp/lib/python3.9/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 8 invalid image filename(s) in x_col=\"filename\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'filename': X_train, 'class': y_train}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'filename': X_test, 'class': y_test}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    decay_rate = 0.1\n",
    "    decay_step = 10\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * math.exp(-decay_rate)\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 00:13:29.168905: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 1.9937 - accuracy: 0.2501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 00:14:03.630807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 41s 374ms/step - loss: 1.9937 - accuracy: 0.2501 - val_loss: 1.5707 - val_accuracy: 0.3964 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.6203 - accuracy: 0.3360 - val_loss: 1.6155 - val_accuracy: 0.3738 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 1.5449 - accuracy: 0.3677 - val_loss: 1.4911 - val_accuracy: 0.4238 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.4913 - accuracy: 0.3936 - val_loss: 1.5175 - val_accuracy: 0.4333 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.4268 - accuracy: 0.4351 - val_loss: 1.4968 - val_accuracy: 0.4262 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.3824 - accuracy: 0.4405 - val_loss: 1.6271 - val_accuracy: 0.3893 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 1.3959 - accuracy: 0.4249 - val_loss: 1.6587 - val_accuracy: 0.3476 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 38s 358ms/step - loss: 1.3325 - accuracy: 0.4596 - val_loss: 1.4255 - val_accuracy: 0.4476 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.2739 - accuracy: 0.4843 - val_loss: 1.5087 - val_accuracy: 0.4357 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.2771 - accuracy: 0.4721 - val_loss: 1.4253 - val_accuracy: 0.4631 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 38s 359ms/step - loss: 1.2218 - accuracy: 0.5016 - val_loss: 1.5912 - val_accuracy: 0.3857 - lr: 9.0484e-04\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 1.1767 - accuracy: 0.5124 - val_loss: 1.3873 - val_accuracy: 0.4952 - lr: 9.0484e-04\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 37s 356ms/step - loss: 1.1630 - accuracy: 0.5175 - val_loss: 1.4043 - val_accuracy: 0.4786 - lr: 9.0484e-04\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 1.1296 - accuracy: 0.5216 - val_loss: 1.4653 - val_accuracy: 0.4440 - lr: 9.0484e-04\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 1.0955 - accuracy: 0.5318 - val_loss: 1.3744 - val_accuracy: 0.4940 - lr: 9.0484e-04\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.1127 - accuracy: 0.5336 - val_loss: 1.3521 - val_accuracy: 0.4929 - lr: 9.0484e-04\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 1.0782 - accuracy: 0.5497 - val_loss: 1.3271 - val_accuracy: 0.5131 - lr: 9.0484e-04\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 1.0602 - accuracy: 0.5512 - val_loss: 1.5642 - val_accuracy: 0.4310 - lr: 9.0484e-04\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 1.0456 - accuracy: 0.5509 - val_loss: 1.3205 - val_accuracy: 0.5345 - lr: 9.0484e-04\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 37s 356ms/step - loss: 0.9939 - accuracy: 0.5879 - val_loss: 1.5550 - val_accuracy: 0.4167 - lr: 9.0484e-04\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.9514 - accuracy: 0.5944 - val_loss: 1.6214 - val_accuracy: 0.3940 - lr: 8.1873e-04\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.9869 - accuracy: 0.5810 - val_loss: 1.4753 - val_accuracy: 0.4619 - lr: 8.1873e-04\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.9367 - accuracy: 0.5942 - val_loss: 1.4077 - val_accuracy: 0.4571 - lr: 8.1873e-04\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 38s 356ms/step - loss: 0.9715 - accuracy: 0.5834 - val_loss: 1.2958 - val_accuracy: 0.5310 - lr: 8.1873e-04\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.9560 - accuracy: 0.5903 - val_loss: 1.2391 - val_accuracy: 0.5560 - lr: 8.1873e-04\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.8933 - accuracy: 0.6103 - val_loss: 1.3553 - val_accuracy: 0.5000 - lr: 8.1873e-04\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.9001 - accuracy: 0.6112 - val_loss: 1.2488 - val_accuracy: 0.5560 - lr: 8.1873e-04\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.9031 - accuracy: 0.6147 - val_loss: 1.4088 - val_accuracy: 0.4940 - lr: 8.1873e-04\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.9001 - accuracy: 0.6168 - val_loss: 1.3681 - val_accuracy: 0.5107 - lr: 8.1873e-04\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.8781 - accuracy: 0.6174 - val_loss: 1.7068 - val_accuracy: 0.3929 - lr: 8.1873e-04\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.8416 - accuracy: 0.6297 - val_loss: 1.3183 - val_accuracy: 0.5345 - lr: 7.4082e-04\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.8372 - accuracy: 0.6389 - val_loss: 1.3789 - val_accuracy: 0.5095 - lr: 7.4082e-04\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.8345 - accuracy: 0.6344 - val_loss: 1.3177 - val_accuracy: 0.5464 - lr: 7.4082e-04\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.8250 - accuracy: 0.6395 - val_loss: 1.4176 - val_accuracy: 0.4881 - lr: 7.4082e-04\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 0.7894 - accuracy: 0.6523 - val_loss: 1.3039 - val_accuracy: 0.5321 - lr: 7.4082e-04\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.7931 - accuracy: 0.6514 - val_loss: 1.3950 - val_accuracy: 0.5298 - lr: 7.4082e-04\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 37s 356ms/step - loss: 0.7882 - accuracy: 0.6541 - val_loss: 1.3223 - val_accuracy: 0.5321 - lr: 7.4082e-04\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 0.8184 - accuracy: 0.6509 - val_loss: 1.2918 - val_accuracy: 0.5560 - lr: 7.4082e-04\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.7771 - accuracy: 0.6705 - val_loss: 1.2603 - val_accuracy: 0.5560 - lr: 7.4082e-04\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 0.7378 - accuracy: 0.6738 - val_loss: 1.2391 - val_accuracy: 0.5702 - lr: 7.4082e-04\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.7307 - accuracy: 0.6780 - val_loss: 1.2373 - val_accuracy: 0.5905 - lr: 6.7032e-04\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 0.7148 - accuracy: 0.6852 - val_loss: 1.4193 - val_accuracy: 0.5202 - lr: 6.7032e-04\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.7219 - accuracy: 0.6837 - val_loss: 1.2237 - val_accuracy: 0.5905 - lr: 6.7032e-04\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.7000 - accuracy: 0.6923 - val_loss: 1.2663 - val_accuracy: 0.5631 - lr: 6.7032e-04\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.7604 - accuracy: 0.6762 - val_loss: 1.2435 - val_accuracy: 0.5798 - lr: 6.7032e-04\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.7361 - accuracy: 0.6885 - val_loss: 1.2656 - val_accuracy: 0.5667 - lr: 6.7032e-04\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 37s 355ms/step - loss: 0.7278 - accuracy: 0.6741 - val_loss: 1.3213 - val_accuracy: 0.5667 - lr: 6.7032e-04\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.6855 - accuracy: 0.6992 - val_loss: 1.2076 - val_accuracy: 0.5940 - lr: 6.7032e-04\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.7049 - accuracy: 0.6819 - val_loss: 1.3545 - val_accuracy: 0.5357 - lr: 6.7032e-04\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 37s 353ms/step - loss: 0.6518 - accuracy: 0.7061 - val_loss: 1.2521 - val_accuracy: 0.5726 - lr: 6.7032e-04\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 00:44:41.212097: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 1.5792 - accuracy: 0.4885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 00:45:18.261479: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 45s 381ms/step - loss: 1.5792 - accuracy: 0.4885 - val_loss: 2.3298 - val_accuracy: 0.4512 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 38s 358ms/step - loss: 0.9577 - accuracy: 0.6127 - val_loss: 1.5381 - val_accuracy: 0.5952 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 38s 355ms/step - loss: 0.7203 - accuracy: 0.6968 - val_loss: 1.4386 - val_accuracy: 0.5750 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.5535 - accuracy: 0.7640 - val_loss: 1.5358 - val_accuracy: 0.5786 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 38s 355ms/step - loss: 0.4448 - accuracy: 0.8045 - val_loss: 1.0579 - val_accuracy: 0.6821 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 38s 357ms/step - loss: 0.3767 - accuracy: 0.8356 - val_loss: 1.1697 - val_accuracy: 0.6655 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 38s 356ms/step - loss: 0.3728 - accuracy: 0.8508 - val_loss: 1.4244 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 37s 354ms/step - loss: 0.3409 - accuracy: 0.8588 - val_loss: 1.2373 - val_accuracy: 0.6929 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 38s 355ms/step - loss: 0.2780 - accuracy: 0.8815 - val_loss: 1.1141 - val_accuracy: 0.7167 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 38s 362ms/step - loss: 0.2168 - accuracy: 0.9078 - val_loss: 1.0346 - val_accuracy: 0.7274 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 39s 367ms/step - loss: 0.2320 - accuracy: 0.9027 - val_loss: 1.9778 - val_accuracy: 0.6095 - lr: 9.0484e-05\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 39s 372ms/step - loss: 0.1939 - accuracy: 0.9209 - val_loss: 1.2311 - val_accuracy: 0.7167 - lr: 9.0484e-05\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 40s 380ms/step - loss: 0.1810 - accuracy: 0.9260 - val_loss: 0.9486 - val_accuracy: 0.7548 - lr: 9.0484e-05\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 40s 382ms/step - loss: 0.2563 - accuracy: 0.9009 - val_loss: 1.2455 - val_accuracy: 0.6786 - lr: 9.0484e-05\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 41s 387ms/step - loss: 0.1874 - accuracy: 0.9242 - val_loss: 1.2070 - val_accuracy: 0.7369 - lr: 9.0484e-05\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 41s 391ms/step - loss: 0.1458 - accuracy: 0.9388 - val_loss: 1.1570 - val_accuracy: 0.7369 - lr: 9.0484e-05\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 42s 396ms/step - loss: 0.1142 - accuracy: 0.9534 - val_loss: 1.1305 - val_accuracy: 0.7536 - lr: 9.0484e-05\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 42s 400ms/step - loss: 0.1577 - accuracy: 0.9406 - val_loss: 1.2420 - val_accuracy: 0.7333 - lr: 9.0484e-05\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 42s 400ms/step - loss: 0.1079 - accuracy: 0.9558 - val_loss: 1.2459 - val_accuracy: 0.7214 - lr: 9.0484e-05\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 42s 401ms/step - loss: 0.1200 - accuracy: 0.9508 - val_loss: 1.2644 - val_accuracy: 0.7119 - lr: 9.0484e-05\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 42s 401ms/step - loss: 0.0779 - accuracy: 0.9678 - val_loss: 1.0397 - val_accuracy: 0.7845 - lr: 8.1873e-05\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 43s 405ms/step - loss: 0.0699 - accuracy: 0.9719 - val_loss: 1.1732 - val_accuracy: 0.7548 - lr: 8.1873e-05\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 43s 407ms/step - loss: 0.0699 - accuracy: 0.9719 - val_loss: 1.3475 - val_accuracy: 0.7381 - lr: 8.1873e-05\n",
      "27/27 [==============================] - 5s 198ms/step - loss: 0.9486 - accuracy: 0.7548\n",
      "Test loss: 0.948584794998169\n",
      "Test accuracy: 0.7547619342803955\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet50 base model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*image_size, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(set(y)), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[lr_scheduler],\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers and apply EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100  # Example: fine-tune starting from layer 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "fine_tuning_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tuning_learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=test_generator,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[lr_scheduler, early_stopping],\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.18      0.29        17\n",
      "           1       0.74      0.80      0.77       159\n",
      "           2       0.85      0.86      0.85       159\n",
      "           3       0.63      0.71      0.67       160\n",
      "           4       0.79      0.76      0.77       164\n",
      "           5       0.80      0.77      0.78       167\n",
      "           6       0.33      0.14      0.20        14\n",
      "\n",
      "    accuracy                           0.75       840\n",
      "   macro avg       0.70      0.60      0.62       840\n",
      "weighted avg       0.75      0.75      0.75       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = [tf.argmax(pred).numpy() for pred in predictions]\n",
    "true_labels = test_generator.classes\n",
    "report = classification_report(true_labels, predicted_labels)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yo/miniforge3/envs/mlp/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"facial_modelling.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
